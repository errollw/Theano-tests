{
 "metadata": {
  "name": "",
  "signature": "sha256:18b8fd4e319795aac82e7f6e9b11d3c7d1a84cc5bce81e5549e16b3803ab0c8c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle\n",
      "import gzip\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "import cv2\n",
      "\n",
      "import random # for making train, valid, and test sets\n",
      "\n",
      "# displaying images inline\n",
      "from IPython.display import Image, display, clear_output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset_dir = \"C:\\Users\\Erroll\\Documents\\columbia_gaze_data_set\\Columbia Gaze Data Set\"\n",
      "debug_dir = \"C:\\Users\\Erroll\\Documents\\columbia_gaze_data_set\\Columbia Gaze Data Set\\debug\"\n",
      "imgs_dir = \"C:\\Users\\Erroll\\Documents\\columbia_gaze_data_set\\Columbia Gaze Data Set\\output\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# from http://www.janeriksolem.net/2009/01/pca-for-images-using-python.html\n",
      "\n",
      "def pca(X):\n",
      "    # Principal Component Analysis\n",
      "    # input: X, each row is a flattened image\n",
      "    # return: projection matrix (with important dimensions first), variance, and mean\n",
      "\n",
      "    #get dimensions\n",
      "    num_data, dim = X.shape\n",
      "\n",
      "    #center data\n",
      "    mean_X = X.mean(axis=0)\n",
      "    for i in range(num_data):\n",
      "      X[i] -= mean_X\n",
      "\n",
      "    #only makes sense to return the first num_data\n",
      "    U,S,V = np.linalg.svd(X, full_matrices=False)\n",
      "    V = V[:num_data]\n",
      "\n",
      "    #return the projection matrix, the variance and the mean\n",
      "    return V,S,mean_X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "os.chdir(imgs_dir)\n",
      "all_imnames = os.listdir(imgs_dir)\n",
      "\n",
      "nolock_fns = [fn for fn in all_imnames if \"nolock\" in fn]\n",
      "lock_fns = set(all_imnames) - set(nolock_fns)\n",
      "num_lock_imgs = len(lock_fns)\n",
      "nolock_fns_balanced = random.sample(nolock_fns, num_lock_imgs)\n",
      "\n",
      "all_imnames_balanced = list(set(nolock_fns_balanced).union(lock_fns))\n",
      "\n",
      "all_ims = [cv2.imread(os.path.join(imgs_dir, imname), cv2.CV_LOAD_IMAGE_GRAYSCALE) for imname in all_imnames_balanced]\n",
      "all_ims_flat = [im.flatten() for im in all_ims]\n",
      "\n",
      "matrix = np.vstack(all_ims_flat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# perform PCA\n",
      "V, S, immean = pca(matrix)\n",
      "\n",
      "mean_img = immean.reshape(24,40)\n",
      "cv2.normalize(mean_img, mean_img, 0, 255, cv2.NORM_MINMAX)\n",
      "\n",
      "cv2.imwrite(os.path.join(debug_dir,\"mean_img.png\"), mean_img)\n",
      "display(Image(os.path.join(debug_dir,\"mean_img.png\")))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAACgAAAAYCAAAAACt7+AcAAACuklEQVQoFRXBO3NbVRAA4N09e/bc\nK18U2Y6wB8PgCflZDB30dBTUdMzQ8AMo6WgpGUoKCsoMIZmE2CBZyHre89hd7O/D39sjfeBuBoDg\nAAgACIiBKITAHGPA30otTVttTd0AHNER0cEREIlYOHCMHPHXkksttVZXAwMzMEAgBEIiCsgpSpQk\n+HMuOefaVE211aaqDsgUIkcOxCHElFIn+NNxHHNtrqXU/KCoumPgKKlLMXHgGKXve/zhONZqrdQy\nHvb7w7FUdYAQY38yTPqJxMgxStfj97k0rbmO+bjdbrYHaxXQUqDJ8GQY+j6JCLN0+G2pNdecm47b\n9Wp70KYGICH009PZkCRKkhSj4Dc557EoUsibf2/vx1bdAAOzzC7en0Y35JS6JPj18ZiLx36IefHm\n7V3WZgBOzHL2wUdzaYexhS6lHr88HDOk4Wwmh5uXL5dja+YAGGM6f/bsoq/r9a5S1/f4xT47v3c2\nP5fd7Z8vbvdazQECh8nl808uT/Jqudoppwl+dmiUZvP50zQuXr94szVVBQqBhqvn1/NuvFss7jPI\nBD/NGvrp+dPzE1u/++vVytzMiYBm19dXM9rdLVebUUOPnxfHNJnOzoYwLv9+fZPNHYAgXX784Vmn\n29V6cyxGgl81RZT+ZJiI75c3b1dZHZDS6dXV+QTzfnscmzpF/K6pO7GIMNl+tfjnv2xO6fRiftoF\nr6VUMwBi/LGpGjgSEbiOu/vNzjycTJ8MKSCoGQIiEeMvtTZVdUcHNy0lFwWSlIQIAB2JKXBg/CPX\nUpua+SN75AhIgRABCSkwc2TBV7nkWps28Afg5g7o+AAAiJBDFBYWXIyljLVWdTN44A7o8AgBiSiw\niERJuM95LKW0auruAI4OAOgAiECBokSJKSX0csxjaaU1NQN3AARHAwACCoFjlCipS/8Dbje7q80l\nXFIAAAAASUVORK5CYII=\n",
       "text": [
        "<IPython.core.display.Image at 0x3ae62e8>"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random.shuffle(all_imnames_balanced)\n",
      "test_imnames = random.sample(set(all_imnames_balanced), 150)\n",
      "valid_imnames = random.sample(set(all_imnames_balanced)-set(test_imnames), 150)\n",
      "train_imnames = list(set(all_imnames_balanced)-set(test_imnames)-set(valid_imnames))\n",
      "random.shuffle(train_imnames)\n",
      "\n",
      "print len(all_imnames_balanced), len(test_imnames), len(valid_imnames), len(train_imnames)\n",
      "\n",
      "train_imgs, train_lbls = [], []\n",
      "valid_imgs, valid_lbls = [], []\n",
      "test_imgs, test_lbls = [], []\n",
      "\n",
      "for imname in train_imnames:\n",
      "    img = cv2.imread(imname, cv2.CV_LOAD_IMAGE_GRAYSCALE).flatten()\n",
      "    coeffs = [np.dot(img - immean, v) for v in V[:32]]\n",
      "    train_imgs.append(np.array(coeffs).astype(np.float32))\n",
      "    train_lbls.append(0 if \"nolock\" in imname else 1)\n",
      "\n",
      "for imname in valid_imnames:\n",
      "    img = cv2.imread(imname, cv2.CV_LOAD_IMAGE_GRAYSCALE).flatten()\n",
      "    coeffs = [np.dot(img - immean, v) for v in V[:32]]\n",
      "    valid_imgs.append(np.array(coeffs).astype(np.float32))\n",
      "    valid_lbls.append(0 if \"nolock\" in imname else 1)\n",
      "    \n",
      "for imname in test_imnames:\n",
      "    img = cv2.imread(imname, cv2.CV_LOAD_IMAGE_GRAYSCALE).flatten()\n",
      "    coeffs = [np.dot(img - immean, v) for v in V[:32]]\n",
      "    test_imgs.append(np.array(coeffs).astype(np.float32))\n",
      "    test_lbls.append(0 if \"nolock\" in imname else 1)\n",
      "    \n",
      "train_set = [np.array(train_imgs), np.array(train_lbls).astype(np.int64)]\n",
      "valid_set = [np.array(valid_imgs), np.array(valid_lbls).astype(np.int64)]\n",
      "test_set= [np.array(test_imgs), np.array(test_lbls).astype(np.int64)]\n",
      "\n",
      "print train_set[0].shape, train_set[1].shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1336 150 150 1036\n",
        "(1036L, 32L)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (1036L,)\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LogisticRegression(object):\n",
      "    \"\"\"Multi-class Logistic Regression Class\n",
      "\n",
      "    The logistic regression is fully described by a weight matrix :math:`W`\n",
      "    and bias vector :math:`b`. Classification is done by projecting data\n",
      "    points onto a set of hyperplanes, the distance to which is used to\n",
      "    determine a class membership probability.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, input, n_in, n_out):\n",
      "        \"\"\" Initialize the parameters of the logistic regression\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: symbolic variable that describes the input of the\n",
      "                      architecture (one minibatch)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: number of input units, the dimension of the space in\n",
      "                     which the datapoints lie\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "                      which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
      "        self.W = theano.shared(value=np.zeros((n_in, n_out), dtype=theano.config.floatX),\n",
      "                                name='W', borrow=True)\n",
      "        # initialize the baises b as a vector of n_out 0s\n",
      "        self.b = theano.shared(value=np.zeros((n_out,), dtype=theano.config.floatX),\n",
      "                               name='b', borrow=True)\n",
      "\n",
      "        # compute vector of class-membership probabilities in symbolic form\n",
      "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
      "\n",
      "        # compute prediction as class whose probability is maximal in\n",
      "        # symbolic form\n",
      "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
      "\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "    def negative_log_likelihood(self, y):\n",
      "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
      "        of this model under a given target distribution.\n",
      "\n",
      "        .. math::\n",
      "\n",
      "            \\frac{1}{|\\mathcal{D}|} \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) =\n",
      "            \\frac{1}{|\\mathcal{D}|} \\sum_{i=0}^{|\\mathcal{D}|} \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\\n",
      "                \\ell (\\theta=\\{W,b\\}, \\mathcal{D})\n",
      "\n",
      "        :type y: theano.tensor.TensorType\n",
      "        :param y: corresponds to a vector that gives for each example the\n",
      "                  correct label\n",
      "\n",
      "        Note: we use the mean instead of the sum so that\n",
      "              the learning rate is less dependent on the batch size\n",
      "        \"\"\"\n",
      "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
      "        # number of examples (call it n) in the minibatch\n",
      "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
      "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
      "        # Log-Probabilities (call it LP) with one row per example and\n",
      "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
      "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
      "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
      "        # the mean (across minibatch examples) of the elements in v,\n",
      "        # i.e., the mean log-likelihood across the minibatch.\n",
      "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
      "\n",
      "    def errors(self, y):\n",
      "        \"\"\"Return a float representing the number of errors in the minibatch\n",
      "        over the total number of examples of the minibatch ; zero one\n",
      "        loss over the size of the minibatch\n",
      "\n",
      "        :type y: theano.tensor.TensorType\n",
      "        :param y: corresponds to a vector that gives for each example the\n",
      "                  correct label\n",
      "        \"\"\"\n",
      "\n",
      "        # check if y has same dimension of y_pred\n",
      "        if y.ndim != self.y_pred.ndim:\n",
      "            raise TypeError('y should have the same shape as self.y_pred',\n",
      "                ('y', target.type, 'y_pred', self.y_pred.type))\n",
      "        # check if y is of the correct datatype\n",
      "        if y.dtype.startswith('int'):\n",
      "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
      "            # represents a mistake in prediction\n",
      "            return T.mean(T.neq(self.y_pred, y))\n",
      "        else:\n",
      "            raise NotImplementedError()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_data():\n",
      "    ''' Loads the dataset\n",
      "\n",
      "    :type dataset: string\n",
      "    :param dataset: the path to the dataset (here MNIST)\n",
      "    '''\n",
      "\n",
      "    #############\n",
      "    # LOAD DATA #\n",
      "    #############\n",
      "\n",
      "    f = file('eyes.save', 'rb')\n",
      "    train_set, valid_set, test_set = cPickle.load(f)\n",
      "    f.close()\n",
      "    \n",
      "    def shared_dataset(data_xy, borrow=True):\n",
      "        \"\"\" Function that loads the dataset into shared variables\n",
      "\n",
      "        The reason we store our dataset in shared variables is to allow\n",
      "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
      "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
      "        is needed (the default behaviour if the data is not in a shared\n",
      "        variable) would lead to a large decrease in performance.\n",
      "        \"\"\"\n",
      "        data_x, data_y = data_xy\n",
      "        shared_x = theano.shared(np.asarray(data_x,\n",
      "                                               dtype=theano.config.floatX),\n",
      "                                 borrow=borrow)\n",
      "        shared_y = theano.shared(np.asarray(data_y,\n",
      "                                               dtype=theano.config.floatX),\n",
      "                                 borrow=borrow)\n",
      "        # When storing data on the GPU it has to be stored as floats\n",
      "        # therefore we will store the labels as ``floatX`` as well\n",
      "        # (``shared_y`` does exactly that). But during our computations\n",
      "        # we need them as ints (we use labels as index, and if they are\n",
      "        # floats it doesn't make sense) therefore instead of returning\n",
      "        # ``shared_y`` we will have to cast it to int. This little hack\n",
      "        # lets ous get around this issue\n",
      "        return shared_x, T.cast(shared_y, 'int32')\n",
      "\n",
      "    test_set_x, test_set_y = shared_dataset(test_set)\n",
      "    valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
      "    train_set_x, train_set_y = shared_dataset(train_set)\n",
      "\n",
      "    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y),\n",
      "            (test_set_x, test_set_y)]\n",
      "    return rval"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sgd_optimization_mnist(learning_rate=0.001, n_epochs=1000,\n",
      "                           dataset='mnist.pkl.gz',\n",
      "                           batch_size=50):\n",
      "    \"\"\"\n",
      "    Demonstrate stochastic gradient descent optimization of a log-linear\n",
      "    model\n",
      "\n",
      "    This is demonstrated on MNIST.\n",
      "\n",
      "    :type learning_rate: float\n",
      "    :param learning_rate: learning rate used (factor for the stochastic\n",
      "                          gradient)\n",
      "\n",
      "    :type n_epochs: int\n",
      "    :param n_epochs: maximal number of epochs to run the optimizer\n",
      "\n",
      "    :type dataset: string\n",
      "    :param dataset: the path of the MNIST dataset file from\n",
      "                 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
      "\n",
      "    \"\"\"\n",
      "    datasets = load_data()\n",
      "\n",
      "    train_set_x, train_set_y = datasets[0]\n",
      "    valid_set_x, valid_set_y = datasets[1]\n",
      "    test_set_x, test_set_y = datasets[2]\n",
      "\n",
      "    # compute number of minibatches for training, validation and testing\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "\n",
      "    ######################\n",
      "    # BUILD ACTUAL MODEL #\n",
      "    ######################\n",
      "    print '... building the model'\n",
      "\n",
      "    # allocate symbolic variables for the data\n",
      "    index = T.lscalar()  # index to a [mini]batch\n",
      "    x = T.matrix('x')  # the data is presented as rasterized images\n",
      "    y = T.ivector('y')  # the labels are presented as 1D vector of\n",
      "                           # [int] labels\n",
      "\n",
      "    # construct the logistic regression class\n",
      "    # Each MNIST image has size 28*28\n",
      "    classifier = LogisticRegression(input=x, n_in=32, n_out=2)\n",
      "\n",
      "    # the cost we minimize during training is the negative log likelihood of\n",
      "    # the model in symbolic format\n",
      "    cost = classifier.negative_log_likelihood(y)\n",
      "\n",
      "    # compiling a Theano function that computes the mistakes that are made by\n",
      "    # the model on a minibatch\n",
      "    test_model = theano.function(inputs=[index],\n",
      "            outputs=classifier.errors(y),\n",
      "            givens={\n",
      "                x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "                y: test_set_y[index * batch_size: (index + 1) * batch_size]})\n",
      "\n",
      "    validate_model = theano.function(inputs=[index],\n",
      "            outputs=classifier.errors(y),\n",
      "            givens={\n",
      "                x: valid_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "                y: valid_set_y[index * batch_size:(index + 1) * batch_size]})\n",
      "\n",
      "    # compute the gradient of cost with respect to theta = (W,b)\n",
      "    g_W = T.grad(cost=cost, wrt=classifier.W)\n",
      "    g_b = T.grad(cost=cost, wrt=classifier.b)\n",
      "\n",
      "    # specify how to update the parameters of the model as a list of\n",
      "    # (variable, update expression) pairs.\n",
      "    updates = [(classifier.W, classifier.W - learning_rate * g_W),\n",
      "               (classifier.b, classifier.b - learning_rate * g_b)]\n",
      "\n",
      "    # compiling a Theano function `train_model` that returns the cost, but in\n",
      "    # the same time updates the parameter of the model based on the rules\n",
      "    # defined in `updates`\n",
      "    train_model = theano.function(inputs=[index],\n",
      "            outputs=cost,\n",
      "            updates=updates,\n",
      "            givens={\n",
      "                x: train_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "                y: train_set_y[index * batch_size:(index + 1) * batch_size]})\n",
      "\n",
      "    ###############\n",
      "    # TRAIN MODEL #\n",
      "    ###############\n",
      "    print '... training the model'\n",
      "    # early-stopping parameters\n",
      "    patience = 5000  # look as this many examples regardless\n",
      "    patience_increase = 2  # wait this much longer when a new best is\n",
      "                                  # found\n",
      "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
      "                                  # considered significant\n",
      "    validation_frequency = min(n_train_batches, patience / 2)\n",
      "                                  # go through this many\n",
      "                                  # minibatche before checking the network\n",
      "                                  # on the validation set; in this case we\n",
      "                                  # check every epoch\n",
      "\n",
      "    best_params = None\n",
      "    best_validation_loss = np.inf\n",
      "    test_score = 0.\n",
      "    start_time = time.clock()\n",
      "\n",
      "    done_looping = False\n",
      "    epoch = 0\n",
      "    while (epoch < n_epochs) and (not done_looping):\n",
      "        epoch = epoch + 1\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "\n",
      "            minibatch_avg_cost = train_model(minibatch_index)\n",
      "            # iteration number\n",
      "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
      "\n",
      "            if (iter + 1) % validation_frequency == 0:\n",
      "                # compute zero-one loss on validation set\n",
      "                validation_losses = [validate_model(i)\n",
      "                                     for i in xrange(n_valid_batches)]\n",
      "                this_validation_loss = np.mean(validation_losses)\n",
      "\n",
      "                print('epoch %i, minibatch %i/%i, validation error %f %%' % \\\n",
      "                    (epoch, minibatch_index + 1, n_train_batches,\n",
      "                    this_validation_loss * 100.))\n",
      "\n",
      "                # if we got the best validation score until now\n",
      "                if this_validation_loss < best_validation_loss:\n",
      "                    #improve patience if loss improvement is good enough\n",
      "                    if this_validation_loss < best_validation_loss *  \\\n",
      "                       improvement_threshold:\n",
      "                        patience = max(patience, iter * patience_increase)\n",
      "\n",
      "                    best_validation_loss = this_validation_loss\n",
      "                    # test it on the test set\n",
      "\n",
      "                    test_losses = [test_model(i)\n",
      "                                   for i in xrange(n_test_batches)]\n",
      "                    test_score = np.mean(test_losses)\n",
      "\n",
      "                    print(('     epoch %i, minibatch %i/%i, test error of best'\n",
      "                       ' model %f %%') %\n",
      "                        (epoch, minibatch_index + 1, n_train_batches,\n",
      "                         test_score * 100.))\n",
      "\n",
      "            if patience <= iter:\n",
      "                done_looping = True\n",
      "                break\n",
      "\n",
      "    end_time = time.clock()\n",
      "    print(('Optimization complete with best validation score of %f %%,'\n",
      "           'with test performance %f %%') %\n",
      "                 (best_validation_loss * 100., test_score * 100.))\n",
      "    print 'The code run for %d epochs, with %f epochs/sec' % (\n",
      "        epoch, 1. * epoch / (end_time - start_time))\n",
      "#     print >> sys.stderr, ('The code for file ' +\n",
      "#                           os.path.split(__file__)[1] +\n",
      "#                           ' ran for %.1fs' % ((end_time - start_time)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "os.chdir(\"C:\\Users\\Erroll\\Documents\\Theano test\")\n",
      "sgd_optimization_mnist()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "... building the model\n",
        "... training the model"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 1, minibatch 20/20, validation error 47.333333 %\n",
        "     epoch 1, minibatch 20/20, test error of best model 50.000000 %\n",
        "epoch 2, minibatch 20/20, validation error 45.333333 %\n",
        "     epoch 2, minibatch 20/20, test error of best model 49.333333 %\n",
        "epoch 3, minibatch 20/20, validation error 44.666667 %\n",
        "     epoch 3, minibatch 20/20, test error of best model 48.666667 %\n",
        "epoch 4, minibatch 20/20, validation error 44.666667 %\n",
        "epoch 5, minibatch 20/20, validation error 44.000000 %\n",
        "     epoch 5, minibatch 20/20, test error of best model 46.666667 %\n",
        "epoch 6, minibatch 20/20, validation error 44.000000 %\n",
        "epoch 7, minibatch 20/20, validation error 42.666667 %\n",
        "     epoch 7, minibatch 20/20, test error of best model 45.333333 %\n",
        "epoch 8, minibatch 20/20, validation error 43.333333 %\n",
        "epoch 9, minibatch 20/20, validation error 38.666667 %\n",
        "     epoch 9, minibatch 20/20, test error of best model 43.333333 %\n",
        "epoch 10, minibatch 20/20, validation error 38.000000 %\n",
        "     epoch 10, minibatch 20/20, test error of best model 43.333333 %\n",
        "epoch 11, minibatch 20/20, validation error 36.000000 %\n",
        "     epoch 11, minibatch 20/20, test error of best model 42.000000 %\n",
        "epoch 12, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 13, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 14, minibatch 20/20, validation error 41.333333 %\n",
        "epoch 15, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 16, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 17, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 18, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 19, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 20, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 21, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 22, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 23, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 24, minibatch 20/20, validation error 41.333333 %\n",
        "epoch 25, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 26, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 27, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 28, minibatch 20/20, validation error 41.333333 %\n",
        "epoch 29, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 30, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 31, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 32, minibatch 20/20, validation error 39.333333 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 33, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 34, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 35, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 36, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 37, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 38, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 39, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 40, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 41, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 42, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 43, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 44, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 45, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 46, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 47, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 48, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 49, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 50, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 51, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 52, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 53, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 54, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 55, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 56, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 57, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 58, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 59, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 60, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 61, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 62, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 63, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 64, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 65, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 66, minibatch 20/20, validation error 38.666667 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 67, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 68, minibatch 20/20, validation error 40.000000 %\n",
        "epoch 69, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 70, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 71, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 72, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 73, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 74, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 75, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 76, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 77, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 78, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 79, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 80, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 81, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 82, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 83, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 84, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 85, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 86, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 87, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 88, minibatch 20/20, validation error 40.000000 %\n",
        "epoch 89, minibatch 20/20, validation error 36.000000 %\n",
        "epoch 90, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 91, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 92, minibatch 20/20, validation error 40.000000 %\n",
        "epoch 93, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 94, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 95, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 96, minibatch 20/20, validation error 40.000000 %\n",
        "epoch 97, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 98, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 99, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 100, minibatch 20/20, validation error 38.666667 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 101, minibatch 20/20, validation error 40.000000 %\n",
        "epoch 102, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 103, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 104, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 105, minibatch 20/20, validation error 40.000000 %\n",
        "epoch 106, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 107, minibatch 20/20, validation error 40.000000 %\n",
        "epoch 108, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 109, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 110, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 111, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 112, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 113, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 114, minibatch 20/20, validation error 40.000000 %\n",
        "epoch 115, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 116, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 117, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 118, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 119, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 120, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 121, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 122, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 123, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 124, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 125, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 126, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 127, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 128, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 129, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 130, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 131, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 132, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 133, minibatch 20/20, validation error 38.000000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 134, minibatch 20/20, validation error 40.000000 %\n",
        "epoch 135, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 136, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 137, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 138, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 139, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 140, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 141, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 142, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 143, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 144, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 145, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 146, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 147, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 148, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 149, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 150, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 151, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 152, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 153, minibatch 20/20, validation error 40.000000 %\n",
        "epoch 154, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 155, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 156, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 157, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 158, minibatch 20/20, validation error 40.000000 %\n",
        "epoch 159, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 160, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 161, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 162, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 163, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 164, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 165, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 166, minibatch 20/20, validation error 37.333333 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 167, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 168, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 169, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 170, minibatch 20/20, validation error 40.666667 %\n",
        "epoch 171, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 172, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 173, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 174, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 175, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 176, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 177, minibatch 20/20, validation error 40.000000 %\n",
        "epoch 178, minibatch 20/20, validation error 36.000000 %\n",
        "epoch 179, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 180, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 181, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 182, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 183, minibatch 20/20, validation error 40.000000 %\n",
        "epoch 184, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 185, minibatch 20/20, validation error 40.666667 %\n",
        "epoch 186, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 187, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 188, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 189, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 190, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 191, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 192, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 193, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 194, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 195, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 196, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 197, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 198, minibatch 20/20, validation error 38.666667 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 199, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 200, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 201, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 202, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 203, minibatch 20/20, validation error 40.666667 %\n",
        "epoch 204, minibatch 20/20, validation error 40.666667 %\n",
        "epoch 205, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 206, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 207, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 208, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 209, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 210, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 211, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 212, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 213, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 214, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 215, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 216, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 217, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 218, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 219, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 220, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 221, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 222, minibatch 20/20, validation error 40.666667 %\n",
        "epoch 223, minibatch 20/20, validation error 40.000000 %\n",
        "epoch 224, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 225, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 226, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 227, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 228, minibatch 20/20, validation error 40.000000 %\n",
        "epoch 229, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 230, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 231, minibatch 20/20, validation error 38.000000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 232, minibatch 20/20, validation error 39.333333 %\n",
        "epoch 233, minibatch 20/20, validation error 40.666667 %\n",
        "epoch 234, minibatch 20/20, validation error 40.666667 %\n",
        "epoch 235, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 236, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 237, minibatch 20/20, validation error 36.666667 %\n",
        "epoch 238, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 239, minibatch 20/20, validation error 40.000000 %\n",
        "epoch 240, minibatch 20/20, validation error 40.000000 %\n",
        "epoch 241, minibatch 20/20, validation error 37.333333 %\n",
        "epoch 242, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 243, minibatch 20/20, validation error 40.000000 %\n",
        "epoch 244, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 245, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 246, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 247, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 248, minibatch 20/20, validation error 38.000000 %\n",
        "epoch 249, minibatch 20/20, validation error 38.666667 %\n",
        "epoch 250, minibatch 20/20, validation error 38.666667 %\n",
        "Optimization complete with best validation score of 36.000000 %,with test performance 42.000000 %\n",
        "The code run for 251 epochs, with 645.566114 epochs/sec\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len([lbl for lbl in train_set[1] if lbl == 1]) / float(len(train_set[1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.142593352483\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print test_set[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'test_set' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-14-e1972ae0d970>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'test_set' is not defined"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "os.chdir(dataset_dir)\n",
      "f = file('eyes.save', 'wb')\n",
      "cPickle.dump([train_set, valid_set, test_set], f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = file('eyes.save', 'rb')\n",
      "train_set, valid_set, test_set = cPickle.load(f)\n",
      "f.close()\n",
      "\n",
      "print train_set[0][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[   98.04509735   165.20336914    84.14346313   164.7141571     60.63867188\n",
        "  -143.12683105   115.08016205   116.33054352    92.64001465   150.8658905\n",
        "   123.24028778   131.93997192   101.08670044   102.91896057   116.98545837\n",
        "    96.657341     178.15522766   114.19303894   130.27334595   138.36187744\n",
        "    98.3967514     64.72272491   176.10845947 -1371.59924316   145.93994141\n",
        "   195.82254028   377.77902222   102.59168243   139.29652405    69.78190613\n",
        "   144.55322266   112.92259979]\n"
       ]
      }
     ],
     "prompt_number": 22
    }
   ],
   "metadata": {}
  }
 ]
}
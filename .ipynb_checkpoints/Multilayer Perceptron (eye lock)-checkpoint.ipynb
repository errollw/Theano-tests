{
 "metadata": {
  "name": "",
  "signature": "sha256:62703a180f978c1e78e4746766ca28fd413f771128b884c2a132cbfef3270957"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle\n",
      "import gzip\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import numpy\n",
      "\n",
      "import theano\n",
      "import theano.tensor as T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LogisticRegression(object):\n",
      "    \"\"\"Multi-class Logistic Regression Class\n",
      "\n",
      "    The logistic regression is fully described by a weight matrix :math:`W`\n",
      "    and bias vector :math:`b`. Classification is done by projecting data\n",
      "    points onto a set of hyperplanes, the distance to which is used to\n",
      "    determine a class membership probability.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, input, n_in, n_out):\n",
      "        \"\"\" Initialize the parameters of the logistic regression\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: symbolic variable that describes the input of the\n",
      "                      architecture (one minibatch)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: number of input units, the dimension of the space in\n",
      "                     which the datapoints lie\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "                      which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
      "        self.W = theano.shared(value=numpy.zeros((n_in, n_out),\n",
      "                                                 dtype=theano.config.floatX),\n",
      "                                name='W', borrow=True)\n",
      "        # initialize the baises b as a vector of n_out 0s\n",
      "        self.b = theano.shared(value=numpy.zeros((n_out,),\n",
      "                                                 dtype=theano.config.floatX),\n",
      "                               name='b', borrow=True)\n",
      "\n",
      "        # compute vector of class-membership probabilities in symbolic form\n",
      "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
      "\n",
      "        # compute prediction as class whose probability is maximal in\n",
      "        # symbolic form\n",
      "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
      "\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "    def negative_log_likelihood(self, y):\n",
      "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
      "        of this model under a given target distribution.\n",
      "\n",
      "        .. math::\n",
      "\n",
      "            \\frac{1}{|\\mathcal{D}|} \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) =\n",
      "            \\frac{1}{|\\mathcal{D}|} \\sum_{i=0}^{|\\mathcal{D}|} \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\\n",
      "                \\ell (\\theta=\\{W,b\\}, \\mathcal{D})\n",
      "\n",
      "        :type y: theano.tensor.TensorType\n",
      "        :param y: corresponds to a vector that gives for each example the\n",
      "                  correct label\n",
      "\n",
      "        Note: we use the mean instead of the sum so that\n",
      "              the learning rate is less dependent on the batch size\n",
      "        \"\"\"\n",
      "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
      "        # number of examples (call it n) in the minibatch\n",
      "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
      "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
      "        # Log-Probabilities (call it LP) with one row per example and\n",
      "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
      "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
      "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
      "        # the mean (across minibatch examples) of the elements in v,\n",
      "        # i.e., the mean log-likelihood across the minibatch.\n",
      "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
      "\n",
      "    def errors(self, y):\n",
      "        \"\"\"Return a float representing the number of errors in the minibatch\n",
      "        over the total number of examples of the minibatch ; zero one\n",
      "        loss over the size of the minibatch\n",
      "\n",
      "        :type y: theano.tensor.TensorType\n",
      "        :param y: corresponds to a vector that gives for each example the\n",
      "                  correct label\n",
      "        \"\"\"\n",
      "\n",
      "        # check if y has same dimension of y_pred\n",
      "        if y.ndim != self.y_pred.ndim:\n",
      "            raise TypeError('y should have the same shape as self.y_pred',\n",
      "                ('y', target.type, 'y_pred', self.y_pred.type))\n",
      "        # check if y is of the correct datatype\n",
      "        if y.dtype.startswith('int'):\n",
      "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
      "            # represents a mistake in prediction\n",
      "            return T.mean(T.neq(self.y_pred, y))\n",
      "        else:\n",
      "            raise NotImplementedError()\n",
      "\n",
      "\n",
      "def load_data(dataset):\n",
      "    ''' Loads the dataset\n",
      "\n",
      "    :type dataset: string\n",
      "    :param dataset: the path to the dataset (here MNIST)\n",
      "    '''\n",
      "\n",
      "    #############\n",
      "    # LOAD DATA #\n",
      "    #############\n",
      "\n",
      "    f = file('eyes.save', 'rb')\n",
      "    train_set, valid_set, test_set = cPickle.load(f)\n",
      "    f.close()\n",
      "    #train_set, valid_set, test_set format: tuple(input, target)\n",
      "    #input is an numpy.ndarray of 2 dimensions (a matrix)\n",
      "    #witch row's correspond to an example. target is a\n",
      "    #numpy.ndarray of 1 dimensions (vector)) that have the same length as\n",
      "    #the number of rows in the input. It should give the target\n",
      "    #target to the example with the same index in the input.\n",
      "\n",
      "    def shared_dataset(data_xy, borrow=True):\n",
      "        \"\"\" Function that loads the dataset into shared variables\n",
      "\n",
      "        The reason we store our dataset in shared variables is to allow\n",
      "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
      "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
      "        is needed (the default behaviour if the data is not in a shared\n",
      "        variable) would lead to a large decrease in performance.\n",
      "        \"\"\"\n",
      "        data_x, data_y = data_xy\n",
      "        shared_x = theano.shared(numpy.asarray(data_x,\n",
      "                                               dtype=theano.config.floatX),\n",
      "                                 borrow=borrow)\n",
      "        shared_y = theano.shared(numpy.asarray(data_y,\n",
      "                                               dtype=theano.config.floatX),\n",
      "                                 borrow=borrow)\n",
      "        # When storing data on the GPU it has to be stored as floats\n",
      "        # therefore we will store the labels as ``floatX`` as well\n",
      "        # (``shared_y`` does exactly that). But during our computations\n",
      "        # we need them as ints (we use labels as index, and if they are\n",
      "        # floats it doesn't make sense) therefore instead of returning\n",
      "        # ``shared_y`` we will have to cast it to int. This little hack\n",
      "        # lets ous get around this issue\n",
      "        return shared_x, T.cast(shared_y, 'int32')\n",
      "\n",
      "    test_set_x, test_set_y = shared_dataset(test_set)\n",
      "    valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
      "    train_set_x, train_set_y = shared_dataset(train_set)\n",
      "\n",
      "    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y),\n",
      "            (test_set_x, test_set_y)]\n",
      "    return rval"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class HiddenLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
      "                 activation=T.tanh):\n",
      "        \"\"\"\n",
      "        Typical hidden layer of a MLP: units are fully-connected and have\n",
      "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
      "        and the bias vector b is of shape (n_out,).\n",
      "\n",
      "        NOTE : The nonlinearity used here is tanh\n",
      "\n",
      "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.dmatrix\n",
      "        :param input: a symbolic tensor of shape (n_examples, n_in)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: dimensionality of input\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of hidden units\n",
      "\n",
      "        :type activation: theano.Op or function\n",
      "        :param activation: Non linearity to be applied in the hidden\n",
      "                           layer\n",
      "        \"\"\"\n",
      "        self.input = input\n",
      "\n",
      "        # `W` is initialized with `W_values` which is uniformely sampled\n",
      "        # from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))\n",
      "        # for tanh activation function\n",
      "        # the output of uniform if converted using asarray to dtype\n",
      "        # theano.config.floatX so that the code is runable on GPU\n",
      "        # Note : optimal initialization of weights is dependent on the\n",
      "        #        activation function used (among other things).\n",
      "        #        For example, results presented in [Xavier10] suggest that you\n",
      "        #        should use 4 times larger initial weights for sigmoid\n",
      "        #        compared to tanh\n",
      "        #        We have no info for other function, so we use the same as\n",
      "        #        tanh.\n",
      "        if W is None:\n",
      "            W_values = numpy.asarray(rng.uniform(\n",
      "                    low=-numpy.sqrt(6. / (n_in + n_out)),\n",
      "                    high=numpy.sqrt(6. / (n_in + n_out)),\n",
      "                    size=(n_in, n_out)), dtype=theano.config.floatX)\n",
      "            if activation == theano.tensor.nnet.sigmoid:\n",
      "                W_values *= 4\n",
      "\n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        if b is None:\n",
      "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
      "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "\n",
      "        lin_output = T.dot(input, self.W) + self.b\n",
      "        self.output = (lin_output if activation is None\n",
      "                       else activation(lin_output))\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "\n",
      "class MLP(object):\n",
      "    \"\"\"Multi-Layer Perceptron Class\n",
      "\n",
      "    A multilayer perceptron is a feedforward artificial neural network model\n",
      "    that has one layer or more of hidden units and nonlinear activations.\n",
      "    Intermediate layers usually have as activation function tanh or the\n",
      "    sigmoid function (defined here by a ``HiddenLayer`` class)  while the\n",
      "    top layer is a softamx layer (defined here by a ``LogisticRegression``\n",
      "    class).\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: symbolic variable that describes the input of the\n",
      "        architecture (one minibatch)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: number of input units, the dimension of the space in\n",
      "        which the datapoints lie\n",
      "\n",
      "        :type n_hidden: int\n",
      "        :param n_hidden: number of hidden units\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "        which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        # Since we are dealing with a one hidden layer MLP, this will translate\n",
      "        # into a HiddenLayer with a tanh activation function connected to the\n",
      "        # LogisticRegression layer; the activation function can be replaced by\n",
      "        # sigmoid or any other nonlinear function\n",
      "        self.hiddenLayer = HiddenLayer(rng=rng, input=input,\n",
      "                                       n_in=n_in, n_out=n_hidden,\n",
      "                                       activation=T.tanh)\n",
      "\n",
      "        # The logistic regression layer gets as input the hidden units\n",
      "        # of the hidden layer\n",
      "        self.logRegressionLayer = LogisticRegression(\n",
      "            input=self.hiddenLayer.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out)\n",
      "\n",
      "        # L1 norm ; one regularization option is to enforce L1 norm to\n",
      "        # be small\n",
      "        self.L1 = abs(self.hiddenLayer.W).sum() \\\n",
      "                + abs(self.logRegressionLayer.W).sum()\n",
      "\n",
      "        # square of L2 norm ; one regularization option is to enforce\n",
      "        # square of L2 norm to be small\n",
      "        self.L2_sqr = (self.hiddenLayer.W ** 2).sum() \\\n",
      "                    + (self.logRegressionLayer.W ** 2).sum()\n",
      "\n",
      "        # negative log likelihood of the MLP is given by the negative\n",
      "        # log likelihood of the output of the model, computed in the\n",
      "        # logistic regression layer\n",
      "        self.negative_log_likelihood = self.logRegressionLayer.negative_log_likelihood\n",
      "        # same holds for the function computing the number of errors\n",
      "        self.errors = self.logRegressionLayer.errors\n",
      "\n",
      "        # the parameters of the model are the parameters of the two layer it is\n",
      "        # made out of\n",
      "        self.params = self.hiddenLayer.params + self.logRegressionLayer.params"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_mlp(learning_rate=0.001, L1_reg=0.00, L2_reg=0.0001, n_epochs=1000,\n",
      "             dataset='mnist.pkl.gz', batch_size=10, n_hidden=200):\n",
      "    \"\"\"\n",
      "    Demonstrate stochastic gradient descent optimization for a multilayer\n",
      "    perceptron\n",
      "\n",
      "    This is demonstrated on MNIST.\n",
      "\n",
      "    :type learning_rate: float\n",
      "    :param learning_rate: learning rate used (factor for the stochastic\n",
      "    gradient\n",
      "\n",
      "    :type L1_reg: float\n",
      "    :param L1_reg: L1-norm's weight when added to the cost (see\n",
      "    regularization)\n",
      "\n",
      "    :type L2_reg: float\n",
      "    :param L2_reg: L2-norm's weight when added to the cost (see\n",
      "    regularization)\n",
      "\n",
      "    :type n_epochs: int\n",
      "    :param n_epochs: maximal number of epochs to run the optimizer\n",
      "\n",
      "    :type dataset: string\n",
      "    :param dataset: the path of the MNIST dataset file from\n",
      "                 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
      "\n",
      "\n",
      "   \"\"\"\n",
      "    datasets = load_data(dataset)\n",
      "\n",
      "    train_set_x, train_set_y = datasets[0]\n",
      "    valid_set_x, valid_set_y = datasets[1]\n",
      "    test_set_x, test_set_y = datasets[2]\n",
      "\n",
      "    # compute number of minibatches for training, validation and testing\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "\n",
      "    ######################\n",
      "    # BUILD ACTUAL MODEL #\n",
      "    ######################\n",
      "    print '... building the model'\n",
      "\n",
      "    # allocate symbolic variables for the data\n",
      "    index = T.lscalar()  # index to a [mini]batch\n",
      "    x = T.matrix('x')  # the data is presented as rasterized images\n",
      "    y = T.ivector('y')  # the labels are presented as 1D vector of\n",
      "                        # [int] labels\n",
      "\n",
      "    rng = numpy.random.RandomState(1234)\n",
      "\n",
      "    # construct the MLP class\n",
      "    classifier = MLP(rng=rng, input=x, n_in=32,\n",
      "                     n_hidden=n_hidden, n_out=2)\n",
      "\n",
      "    # the cost we minimize during training is the negative log likelihood of\n",
      "    # the model plus the regularization terms (L1 and L2); cost is expressed\n",
      "    # here symbolically\n",
      "    cost = classifier.negative_log_likelihood(y) \\\n",
      "         + L1_reg * classifier.L1 \\\n",
      "         + L2_reg * classifier.L2_sqr\n",
      "\n",
      "    # compiling a Theano function that computes the mistakes that are made\n",
      "    # by the model on a minibatch\n",
      "    test_model = theano.function(inputs=[index],\n",
      "            outputs=classifier.errors(y),\n",
      "            givens={\n",
      "                x: test_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "                y: test_set_y[index * batch_size:(index + 1) * batch_size]})\n",
      "\n",
      "    validate_model = theano.function(inputs=[index],\n",
      "            outputs=classifier.errors(y),\n",
      "            givens={\n",
      "                x: valid_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "                y: valid_set_y[index * batch_size:(index + 1) * batch_size]})\n",
      "\n",
      "    # compute the gradient of cost with respect to theta (sotred in params)\n",
      "    # the resulting gradients will be stored in a list gparams\n",
      "    gparams = []\n",
      "    for param in classifier.params:\n",
      "        gparam = T.grad(cost, param)\n",
      "        gparams.append(gparam)\n",
      "\n",
      "    # specify how to update the parameters of the model as a list of\n",
      "    # (variable, update expression) pairs\n",
      "    updates = []\n",
      "    # given two list the zip A = [a1, a2, a3, a4] and B = [b1, b2, b3, b4] of\n",
      "    # same length, zip generates a list C of same size, where each element\n",
      "    # is a pair formed from the two lists :\n",
      "    #    C = [(a1, b1), (a2, b2), (a3, b3), (a4, b4)]\n",
      "    for param, gparam in zip(classifier.params, gparams):\n",
      "        updates.append((param, param - learning_rate * gparam))\n",
      "\n",
      "    # compiling a Theano function `train_model` that returns the cost, but\n",
      "    # in the same time updates the parameter of the model based on the rules\n",
      "    # defined in `updates`\n",
      "    train_model = theano.function(inputs=[index], outputs=cost,\n",
      "            updates=updates,\n",
      "            givens={\n",
      "                x: train_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "                y: train_set_y[index * batch_size:(index + 1) * batch_size]})\n",
      "\n",
      "    ###############\n",
      "    # TRAIN MODEL #\n",
      "    ###############\n",
      "    print '... training'\n",
      "\n",
      "    # early-stopping parameters\n",
      "    patience = 10000  # look as this many examples regardless\n",
      "    patience_increase = 2  # wait this much longer when a new best is\n",
      "                           # found\n",
      "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
      "                                   # considered significant\n",
      "    validation_frequency = min(n_train_batches, patience / 2)\n",
      "                                  # go through this many\n",
      "                                  # minibatche before checking the network\n",
      "                                  # on the validation set; in this case we\n",
      "                                  # check every epoch\n",
      "\n",
      "    best_params = None\n",
      "    best_validation_loss = numpy.inf\n",
      "    best_iter = 0\n",
      "    test_score = 0.\n",
      "    start_time = time.clock()\n",
      "\n",
      "    epoch = 0\n",
      "    done_looping = False\n",
      "\n",
      "    while (epoch < n_epochs) and (not done_looping):\n",
      "        epoch = epoch + 1\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "\n",
      "            minibatch_avg_cost = train_model(minibatch_index)\n",
      "            # iteration number\n",
      "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
      "\n",
      "            if (iter + 1) % validation_frequency == 0:\n",
      "                # compute zero-one loss on validation set\n",
      "                validation_losses = [validate_model(i) for i\n",
      "                                     in xrange(n_valid_batches)]\n",
      "                this_validation_loss = numpy.mean(validation_losses)\n",
      "\n",
      "                print('epoch %i, minibatch %i/%i, validation error %f %%' %\n",
      "                     (epoch, minibatch_index + 1, n_train_batches,\n",
      "                      this_validation_loss * 100.))\n",
      "\n",
      "                # if we got the best validation score until now\n",
      "                if this_validation_loss < best_validation_loss:\n",
      "                    #improve patience if loss improvement is good enough\n",
      "                    if this_validation_loss < best_validation_loss *  \\\n",
      "                           improvement_threshold:\n",
      "                        patience = max(patience, iter * patience_increase)\n",
      "\n",
      "                    best_validation_loss = this_validation_loss\n",
      "                    best_iter = iter\n",
      "\n",
      "                    # test it on the test set\n",
      "                    test_losses = [test_model(i) for i\n",
      "                                   in xrange(n_test_batches)]\n",
      "                    test_score = numpy.mean(test_losses)\n",
      "\n",
      "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
      "                           'best model %f %%') %\n",
      "                          (epoch, minibatch_index + 1, n_train_batches,\n",
      "                           test_score * 100.))\n",
      "\n",
      "            if patience <= iter:\n",
      "                    done_looping = True\n",
      "                    break\n",
      "\n",
      "    end_time = time.clock()\n",
      "    print(('Optimization complete. Best validation score of %f %% '\n",
      "           'obtained at iteration %i, with test performance %f %%') %\n",
      "          (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
      "#     print >> sys.stderr, ('The code for file ' +\n",
      "#                           os.path.split(__file__)[1] +\n",
      "#                           ' ran for %.2fm' % ((end_time - start_time) / 60.))\n",
      "\n",
      "    return classifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier = test_mlp(n_epochs=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "... building the model\n",
        "... training"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 1, minibatch 103/103, validation error 36.000000 %\n",
        "     epoch 1, minibatch 103/103, test error of best model 38.000000 %\n",
        "epoch 2, minibatch 103/103, validation error 34.666667 %\n",
        "     epoch 2, minibatch 103/103, test error of best model 32.000000 %\n",
        "epoch 3, minibatch 103/103, validation error 35.333333 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 4, minibatch 103/103, validation error 35.333333 %\n",
        "epoch 5, minibatch 103/103, validation error 33.333333 %\n",
        "     epoch 5, minibatch 103/103, test error of best model 25.333333 %\n",
        "epoch 6, minibatch 103/103, validation error 32.666667 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "     epoch 6, minibatch 103/103, test error of best model 29.333333 %\n",
        "epoch 7, minibatch 103/103, validation error 31.333333 %\n",
        "     epoch 7, minibatch 103/103, test error of best model 28.000000 %\n",
        "epoch 8, minibatch 103/103, validation error 34.000000 %\n",
        "epoch 9, minibatch 103/103, validation error 34.000000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 10, minibatch 103/103, validation error 30.666667 %\n",
        "     epoch 10, minibatch 103/103, test error of best model 27.333333 %\n",
        "epoch 11, minibatch 103/103, validation error 29.333333 %\n",
        "     epoch 11, minibatch 103/103, test error of best model 28.666667 %\n",
        "epoch 12, minibatch 103/103, validation error 32.000000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 13, minibatch 103/103, validation error 33.333333 %\n",
        "epoch 14, minibatch 103/103, validation error 32.000000 %\n",
        "epoch 15, minibatch 103/103, validation error 34.000000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 16, minibatch 103/103, validation error 32.000000 %\n",
        "epoch 17, minibatch 103/103, validation error 34.666667 %\n",
        "epoch 18, minibatch 103/103, validation error 34.666667 %\n",
        "epoch 19, minibatch 103/103, validation error 32.666667 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 20, minibatch 103/103, validation error 33.333333 %\n",
        "epoch 21, minibatch 103/103, validation error 33.333333 %\n",
        "epoch 22, minibatch 103/103, validation error 33.333333 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 23, minibatch 103/103, validation error 34.666667 %\n",
        "epoch 24, minibatch 103/103, validation error 33.333333 %\n",
        "epoch 25, minibatch 103/103, validation error 37.333333 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 26, minibatch 103/103, validation error 33.333333 %\n",
        "epoch 27, minibatch 103/103, validation error 34.000000 %\n",
        "epoch 28, minibatch 103/103, validation error 29.333333 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "     epoch 28, minibatch 103/103, test error of best model 22.666667 %\n",
        "epoch 29, minibatch 103/103, validation error 32.666667 %\n",
        "epoch 30, minibatch 103/103, validation error 31.333333 %\n",
        "epoch 31, minibatch 103/103, validation error 32.000000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 32, minibatch 103/103, validation error 31.333333 %\n",
        "epoch 33, minibatch 103/103, validation error 30.666667 %\n",
        "epoch 34, minibatch 103/103, validation error 30.666667 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 35, minibatch 103/103, validation error 32.666667 %\n",
        "epoch 36, minibatch 103/103, validation error 32.666667 %\n",
        "epoch 37, minibatch 103/103, validation error 32.666667 %\n",
        "epoch 38, minibatch 103/103, validation error 29.333333 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 39, minibatch 103/103, validation error 30.666667 %\n",
        "epoch 40, minibatch 103/103, validation error 28.666667 %\n",
        "     epoch 40, minibatch 103/103, test error of best model 26.000000 %\n",
        "epoch 41, minibatch 103/103, validation error 32.000000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 42, minibatch 103/103, validation error 30.666667 %\n",
        "epoch 43, minibatch 103/103, validation error 28.000000 %\n",
        "     epoch 43, minibatch 103/103, test error of best model 21.333333 %\n",
        "epoch 44, minibatch 103/103, validation error 28.666667 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 45, minibatch 103/103, validation error 30.666667 %\n",
        "epoch 46, minibatch 103/103, validation error 32.000000 %\n",
        "epoch 47, minibatch 103/103, validation error 32.000000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 48, minibatch 103/103, validation error 28.666667 %\n",
        "epoch 49, minibatch 103/103, validation error 29.333333 %\n",
        "epoch 50, minibatch 103/103, validation error 31.333333 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 51, minibatch 103/103, validation error 34.666667 %\n",
        "epoch 52, minibatch 103/103, validation error 30.666667 %\n",
        "epoch 53, minibatch 103/103, validation error 34.666667 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 54, minibatch 103/103, validation error 32.666667 %\n",
        "epoch 55, minibatch 103/103, validation error 32.000000 %\n",
        "epoch 56, minibatch 103/103, validation error 32.666667 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 57, minibatch 103/103, validation error 30.000000 %\n",
        "epoch 58, minibatch 103/103, validation error 32.000000 %\n",
        "epoch 59, minibatch 103/103, validation error 30.666667 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 60, minibatch 103/103, validation error 31.333333 %\n",
        "epoch 61, minibatch 103/103, validation error 32.666667 %\n",
        "epoch 62, minibatch 103/103, validation error 32.666667 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 63, minibatch 103/103, validation error 33.333333 %\n",
        "epoch 64, minibatch 103/103, validation error 29.333333 %\n",
        "epoch 65, minibatch 103/103, validation error 30.666667 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 66, minibatch 103/103, validation error 30.000000 %\n",
        "epoch 67, minibatch 103/103, validation error 30.666667 %\n",
        "epoch 68, minibatch 103/103, validation error 32.000000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 69, minibatch 103/103, validation error 32.666667 %\n",
        "epoch 70, minibatch 103/103, validation error 30.000000 %\n",
        "epoch 71, minibatch 103/103, validation error 34.000000 %\n",
        "epoch 72, minibatch 103/103, validation error 33.333333 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 73, minibatch 103/103, validation error 32.000000 %\n",
        "epoch 74, minibatch 103/103, validation error 32.666667 %\n",
        "epoch 75, minibatch 103/103, validation error 32.000000 %\n",
        "epoch 76, minibatch 103/103, validation error 34.000000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 77, minibatch 103/103, validation error 35.333333 %\n",
        "epoch 78, minibatch 103/103, validation error 35.333333 %\n",
        "epoch 79, minibatch 103/103, validation error 33.333333 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 80, minibatch 103/103, validation error 30.666667 %\n",
        "epoch 81, minibatch 103/103, validation error 30.000000 %\n",
        "epoch 82, minibatch 103/103, validation error 30.000000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 83, minibatch 103/103, validation error 30.666667 %\n",
        "epoch 84, minibatch 103/103, validation error 31.333333 %\n",
        "epoch 85, minibatch 103/103, validation error 34.666667 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 86, minibatch 103/103, validation error 32.000000 %\n",
        "epoch 87, minibatch 103/103, validation error 30.000000 %\n",
        "epoch 88, minibatch 103/103, validation error 33.333333 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 89, minibatch 103/103, validation error 32.000000 %\n",
        "epoch 90, minibatch 103/103, validation error 30.666667 %\n",
        "epoch 91, minibatch 103/103, validation error 30.000000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 92, minibatch 103/103, validation error 32.000000 %\n",
        "epoch 93, minibatch 103/103, validation error 32.666667 %\n",
        "epoch 94, minibatch 103/103, validation error 35.333333 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 95, minibatch 103/103, validation error 32.000000 %\n",
        "epoch 96, minibatch 103/103, validation error 34.000000 %\n",
        "epoch 97, minibatch 103/103, validation error 38.000000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Optimization complete. Best validation score of 28.000000 % obtained at iteration 4429, with test performance 21.333333 %\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "b1 = classifier.hiddenLayer.b.get_value()\n",
      "w1 = classifier.hiddenLayer.W.get_value()\n",
      "b2 = classifier.logRegressionLayer.b.get_value()\n",
      "w2 = classifier.logRegressionLayer.W.get_value()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Softmax.0\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cv2\n",
      "import os\n",
      "import random\n",
      "\n",
      "# displaying images inline\n",
      "from IPython.display import Image, display, clear_output\n",
      "\n",
      "f = file('princoms.save', 'rb')\n",
      "V, immean = cPickle.load(f)\n",
      "f.close()\n",
      "\n",
      "imgs_dir = r\"C:\\Users\\Erroll\\Documents\\columbia_gaze_data_set\\Columbia Gaze Data Set\\output\"\n",
      "\n",
      "all_imnames = os.listdir(imgs_dir)\n",
      "\n",
      "# make balanced set for training\n",
      "nolock_fns = [fn for fn in all_imnames if \"nolock\" in fn]\n",
      "lock_fns = random.sample(set(all_imnames) - set(nolock_fns), 8)\n",
      "num_lock_imgs = len(lock_fns)\n",
      "nolock_fns_balanced = random.sample(nolock_fns, num_lock_imgs)\n",
      "\n",
      "all_imnames_balanced = list(nolock_fns_balanced + lock_fns)\n",
      "\n",
      "print all_imnames_balanced\n",
      "\n",
      "test_imgs = [cv2.imread(os.path.join(imgs_dir,i), cv2.CV_LOAD_IMAGE_GRAYSCALE) for i in all_imnames_balanced]\n",
      "\n",
      "test_imgs_small = [cv2.resize(img, (40,24)) for img in test_imgs]\n",
      "\n",
      "\n",
      "coeffs = [[numpy.dot(img.flatten() - immean, v) for v in V[:32]] for img in test_imgs]\n",
      "\n",
      "for idx in range(len(coeffs)):\n",
      "    x = coeffs[idx]\n",
      "    h = numpy.tanh(numpy.dot(x,w1) + b1)\n",
      "\n",
      "    print b2 + numpy.dot(h,w2), numpy.argmax(b2 + numpy.dot(h,w2))\n",
      "\n",
      "\n",
      "\n",
      "cv2.imwrite(\"img.png\", numpy.hstack(test_imgs))\n",
      "display(Image(\"img.png\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['nolock_21428.jpg', 'nolock_9281.jpg', 'nolock_12956.jpg', 'nolock_23493.jpg', 'nolock_8688.jpg', 'nolock_15965.jpg', 'nolock_9620.jpg', 'nolock_6988.jpg', 'lock_15329.jpg', 'lock_16999.jpg', 'lock_23299.jpg', 'lock_1041.jpg', 'lock_22020.jpg', 'lock_18676.jpg', 'lock_11953.jpg', 'lock_617.jpg']\n",
        "[ 4.00829803 -4.00829803]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0\n",
        "[ 3.21550985 -3.21550985] 0\n",
        "[ 3.97943633 -3.97943633] 0\n",
        "[ 1.82991792 -1.82991792] 0\n",
        "[ 2.95754648 -2.95754648] 0\n",
        "[ 3.24366745 -3.24366745] 0\n",
        "[ 4.45624695 -4.45624695] 0\n",
        "[ 4.08335276 -4.08335276] 0\n",
        "[ 2.32262366 -2.32262366] 0\n",
        "[ 3.70955713 -3.70955713] 0\n",
        "[ 3.81028188 -3.81028188] 0\n",
        "[ 0.38216913 -0.38216913] 0\n",
        "[ 3.14102431 -3.14102431] 0\n",
        "[ 3.58835163 -3.58835163] 0\n",
        "[-0.17614886  0.17614886] 1\n",
        "[ 1.39240176 -1.39240176] 0\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAAYCAAAAABPDBP4AAAgAElEQVRoBQ3BB7CuaWEY5re/X/2/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\nAZUMOw1MhvP0AAAAAElFTkSuQmCC\n",
       "text": [
        "<IPython.core.display.Image at 0xcea7128>"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print os.listdir('.')\n",
      "f = file('eyes.save', 'rb')\n",
      "train_set, valid_set, test_set = cPickle.load(f)\n",
      "f.close()\n",
      "\n",
      "print len([lbl for lbl in train_set[1] if lbl == 1]) / float(len(train_set[1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['.ipynb_checkpoints', 'convolutional_mlp.py', 'data', 'eyes.save', 'img.png', 'Logistic Regression (eye lock).ipynb', 'Logistic Regression.ipynb', 'logistic_sgd.py', 'logistic_sgd.pyc', 'mlp.py', 'mlp.pyc', 'mnist.pkl.gz', 'Multilayer Perceptron (eye lock).ipynb', 'Multilayer Perceptron.ipynb']\n",
        "0.142593352483"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 28
    }
   ],
   "metadata": {}
  }
 ]
}